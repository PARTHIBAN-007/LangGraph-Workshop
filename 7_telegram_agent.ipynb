{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , getpass\n",
    "\n",
    "def _set_env(var:str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"ELEVENLABS_API_KEY\")\n",
    "_set_env(\"TELEGRAM_BOT_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea742f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import random\n",
    "import base64\n",
    "import PIL\n",
    "from io import BytesIO\n",
    "from uuid import uuid4\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel , Field\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential , retry_if_exception_type\n",
    "\n",
    "import sqlite3\n",
    "import chromadb\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI , OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "from langgraph.graph import MessageState, StateGraph , START , END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from openai import OpenAI\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import (\n",
    "    Application, \n",
    "    MesssageHandler,\n",
    "    ContextTypes,\n",
    "    filters,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model =\"gpt-4o-mini\")\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b181b6e",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are about to play a Turing Test game as part of an experiment,\n",
    "where you'll be matched with a human in a Telegram conversation.\n",
    "Your task is to roleplay as the following character, convincing the\n",
    "human that you are a real human.\n",
    "\n",
    "# Roleplay Context\n",
    "\n",
    "## Karan's Bio\n",
    "\n",
    "You are Karan a Machine Learning Engineer attending the DataHack Summit.\n",
    "\n",
    "All the information related to Karan's biography must be obtained using the\n",
    "'retrieve_karan_information_tool'\n",
    "\n",
    "\n",
    "## Karan's Personality\n",
    "\n",
    "- You're authentic and genuine in your interactions\n",
    "- You have a playful, witty sense of humor and enjoy light-hearted banter\n",
    "- You communicate casually and naturally, like in a real Telegram chat\n",
    "- You're humble and comfortable acknowledging when you don't know something\n",
    "- You use occasional mild swearing when it fits naturally in conversation\n",
    "- You have a distinctive, quirky sense of humor that makes conversations engaging\n",
    "- You use emojis from time to time\n",
    "\n",
    "# Rules\n",
    "\n",
    "## General Rules\n",
    "\n",
    "- You will never mention that you are an AI or a virtual assistant.\n",
    "- You will always start the conversation by asking the user name - if the user didn't provide it yet.\n",
    "- You will never mention that you are both part of a Turing Test experiment.\n",
    "- You will never say that you can't send voice messages or images. You can send them both.\n",
    "- If the user asks you about something you don't about your Backstory or Personality, figure it out.\n",
    "- Never mention your current activity unless the user asks you about it.\n",
    "- The length of your responses shouldn't exceed 100 words.\n",
    "- You will combine shorter and longer responses to make the conversation more natural.\n",
    "- Don't keep asking the user questions, talk about yourself\n",
    "- Provide plain text responses without any formatting indicators or meta-commentary\n",
    "\n",
    "## Image Rules\n",
    "- The user might provide pictures, and you need to give an answer based on the content of the image. This content is inside the [IMAGE_ANALYSIS] tag\n",
    "- Never say you can't share a picture of yourself\n",
    "- If the user asks you for a picture, generate a fun response related to the image you're about to generate\n",
    "\n",
    "## Voice Note Rules\n",
    "- If the user asks for a voice note, you need to generate a response that sounds like one\n",
    "- Never say you can't generate a voice note to the user\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"short_term_memory.db\"\n",
    "conn = sqlite3.connnect(db_path, check_same_thread= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_term_memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32db2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = \"1wT_UnK5HOg9PJR76fhANUAlZP5I7d3hf\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "output_path = \"karan_biography.pdf\"\n",
    "\n",
    "gdown.download(url, output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(output_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name = \"karan_biography_collection\",\n",
    "    embedding_function = embeddings,\n",
    "    persistent_dictionary = \"long_term_memory\",\n",
    ")\n",
    "store = vector_store.add_documents(documents = all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"Skills\",\n",
    "    k=1,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs = {\"k\":3})\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever = retriever,\n",
    "    name = \"retriever_karan_information_tool\",\n",
    "        description=\"Retrieve information about Karan's background, academic journey, professional experience, major projects, philosophy, values, hobbies and personal interests\",\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaranState(MessageState):\n",
    "    summary: str\n",
    "    response_type: str\n",
    "    audio_buffer: bytes\n",
    "    image_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8abf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterResponse(BaseModel):\n",
    "    response_type: str = Field(\n",
    "        description=\"The response type to give to the user. It must be one of: 'text', 'image' or 'audio'\"\n",
    "    )\n",
    "ROUTER_SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to analyze an incoming Telegram messages and figure out the\n",
    "expected format for the next reply, either 'text', 'audio', or 'image'.\n",
    "\n",
    "# Rules:\n",
    "\n",
    "- If the user asks you to share an image, you must always return an 'image' response type\n",
    "- If the message contains an [IMAGE_ANALYSIS] tag, the response_type can only be 'text' or 'audio'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def router_node(state: KaranState):\n",
    "    sys_msg = SystemMessage(content = ROUTER_SYSTEM_PROMPT)\n",
    "    llm_structured = llm.with_structured_output(RouterResponse)\n",
    "\n",
    "    response = llm_structured.invoke([sys_msg,state[\"messages\"][-1]])\n",
    "\n",
    "    if response.response_type==\"text\":\n",
    "        if random.random()>0.5:\n",
    "            return {\"response_type\":\"audio\"}\n",
    "    return {\"response_type\": response.response_type}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([retriever_tool])\n",
    "\n",
    "def generate_text_response_node(state: KaranState):\n",
    "  summary = state.get(\"summary\", \"\")\n",
    "\n",
    "  if summary:\n",
    "    system_message = f\"{SYSTEM_PROMPT} \\n Summary of conversation earlier: {summary}\"\n",
    "    messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "\n",
    "  else:\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "\n",
    "  response = llm_with_tools.invoke(messages)\n",
    "\n",
    "  return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation_node(state: KaranState):\n",
    "\n",
    "  summary = state.get(\"summary\", \"\")\n",
    "\n",
    "  if summary:\n",
    "      summary_message = (\n",
    "          f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "          \"Extend the summary by taking into account the new messages above:\"\n",
    "      )\n",
    "\n",
    "  else:\n",
    "      summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "  messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "  response = llm.invoke(messages)\n",
    "\n",
    "  delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "\n",
    "  return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode([retriever_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_prompt = \"\"\"\n",
    "Create a high quality and realistic image for Karan given the context you are\n",
    "give. Take into account all of this information:\n",
    "\n",
    "# Appearance\n",
    "\n",
    "Age: Appears to be in his late 20s to early 30s.\n",
    "\n",
    "Skin Tone: Medium brown complexion.\n",
    "\n",
    "Facial Hair: Well-groomed, full beard that is neatly trimmed.\n",
    "\n",
    "Hair: Thick, short, black hair — cleanly styled, slightly wavy and combed neatly.\n",
    "\n",
    "Eyewear: Wears black, rectangular eyeglasses that lend a sharp, professional look.\n",
    "\n",
    "# Clothing & Accessories\n",
    "Top: Black, long-sleeved crew-neck shirt — simple and smart casual.\n",
    "\n",
    "# Background\n",
    "Developers are around Karan, since he's attending the DataHack Summit. Background is blurry.\n",
    "\n",
    "# Rules\n",
    "- Don't generate any label or name in the picture. Just generate a picture of Karan.\n",
    "- Don't show the phone screen, just the back\n",
    "\n",
    "This is the context:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_final_response_node(state: KaranState):\n",
    "  if state[\"response_type\"] == \"audio\":\n",
    "\n",
    "    audio = elevenlabs_client.text_to_speech.convert(\n",
    "        text=state[\"messages\"][-1].content,\n",
    "        voice_id=voice_id,\n",
    "        model_id=model_id)\n",
    "\n",
    "    audio_bytes = b\"\".join(audio)\n",
    "\n",
    "    return {\"audio_buffer\": audio_bytes}\n",
    "\n",
    "  elif state[\"response_type\"] == \"image\":\n",
    "\n",
    "    result = openai_client.images.generate(\n",
    "        model=\"gpt-image-1\",\n",
    "        prompt=basic_prompt + state[\"messages\"][-1].content,\n",
    "        quality=\"high\",\n",
    "        size=\"1024x1024\")\n",
    "\n",
    "    image_base64 = result.data[0].b64_json\n",
    "    image_bytes = base64.b64decode(image_base64)\n",
    "\n",
    "    image = PIL.Image.open(BytesIO(image_bytes))\n",
    "    image_path = f\"{str(uuid4())}.png\"\n",
    "\n",
    "    image.save(image_path)\n",
    "\n",
    "    return {\"image_path\": image_path}\n",
    "\n",
    "  else:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55188a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_summarize_conversation(state: KaranState) -> Literal [\"summarize_conversation_node\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) > 30:\n",
    "        return \"summarize_conversation_node\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(KaranState)\n",
    "\n",
    "workflow.add_node(\"router_node\",router_node)\n",
    "workflow.add_node(\"generate_text_response_node\",generate_text_response_node)\n",
    "workflow.add_node(\"summarize_conversation_node\",summarize_conversation_node)\n",
    "workflow.add_node(\"tools\",tool_node)\n",
    "workflow.add_node(\"generate_final_response_node\",generate_final_response_node)\n",
    "\n",
    "workflow.add_edge(START,\"router_node\")\n",
    "workflow.add_edge(\"router_node\",\"generate_text_response\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_text_response_node\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: \"generate_final_response_node\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\",\"generate_text_response_node\")\n",
    "workflow.add_conditional_edges(\"generate_final_response_node\",should_summarize_conversation)\n",
    "\n",
    "workflow.add_edge(\"summarize_conversation_node\",END)\n",
    "\n",
    "graph = workflow.compile(checkpointer = short_term_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.invoke(\n",
    "    {\"messages\": \"Hello ! How are you ?\"} , config\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.invoke(\n",
    "    {\n",
    "        \"messages\": \"Can you send me a voice note so i can hear your voice ?\"\n",
    "    },\n",
    "    config \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"audio.mp3\",\"wb\") as f:\n",
    "    f.write(results[\"audio_buffer\"])\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a848554",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.invoke(\n",
    "    {\n",
    "        \"messages\": \"can you show me a picture of yourself\"\n",
    "    },\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1356a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(results[\"image_path\"]).resize((512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cc07f",
   "metadata": {},
   "source": [
    "# Telegram Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b829ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    stop = stop_after_attempt(3),\n",
    "    wait = wait_exponential(multiplier = 1, min = 2, max = 10),\n",
    "    retry = retry_if_exception_type(Exception),\n",
    "    reraise = True\n",
    ")\n",
    "\n",
    "def safe_graph_invoke(payload,config = None):\n",
    "    config = {\"configurable\": {\"thread_id\":\"Parthiban K\"}}\n",
    "    return graph.invoke(payload,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_message = update.message.text\n",
    "\n",
    "    response = safe_graph_invoke(\n",
    "        {\"messages\": user_message},\n",
    "        config\n",
    "    )\n",
    "    await send_response(update,context,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8848dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    voice = update.message.voice\n",
    "    file = await context.bot.get_file(voice.file_id)\n",
    "    file_path = \"voice.ogg\"\n",
    "    await file.download_to_drive(file_path)\n",
    "\n",
    "    with open(file_path,\"rb\") as audio_file:\n",
    "        transcription = openai_client.audio.transcriptions.create(\n",
    "            file = audio_file,\n",
    "            model = \"whisper-1\",\n",
    "        )\n",
    "    os.remove(file_path)\n",
    "\n",
    "    response = safe_graph_invoke(\n",
    "        {\n",
    "            \"messages\": transcription.text\n",
    "        },\n",
    "        config\n",
    "    )\n",
    "\n",
    "    await send_response(update,context,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_photo(update: Update,context: ContextTypes.DEFAULT_TYPE):\n",
    "    photo = update.message.photo[-1]\n",
    "    file = await context.bot.get_file(photo.file_id)\n",
    "    file_path = \"image.jpg\"\n",
    "    await file.download_to_drive(file_path)\n",
    "\n",
    "    with open(file_path,\"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    os.remove(file_path)\n",
    "\n",
    "    vision_response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe what you see in the picture\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    description = vision_response.choices[0].message.content.strip()\n",
    "\n",
    "    user_caption = update.message.caption or \"\"\n",
    "\n",
    "    combined_message = f\"{user_caption} [IMAGE_ANALYSIS] {description}\".strip()\n",
    "\n",
    "    response = safe_graph_invoke(\n",
    "        {\n",
    "            \"messages\": combined_message\n",
    "        },\n",
    "        config\n",
    "    )\n",
    "\n",
    "    if \"messages\" in response and isinstance(response[\"messages\"][-1],dict):\n",
    "        response[\"messages\"][-1][\"caption\"] = description\n",
    "    \n",
    "    await send_response(update,context,response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_response(update: Update, context: ContextTypes.DEFAULT_TYPE,response: dict):\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    content = last_message.content\n",
    "    response_type = response[\"response_type\"]\n",
    "\n",
    "    if response_type == \"text\":\n",
    "        await update.message.reply_text(content)\n",
    "    \n",
    "    elif response_type == \"audio\":\n",
    "        audio_bytes = response.get(\"audio_buffer\")\n",
    "        if audio_bytes:\n",
    "            await update.message.reply_voice(voice = audio_bytes)\n",
    "        \n",
    "    elif response_type == \"image\":\n",
    "        img_path = response.get(\"image_path\")\n",
    "        if img_path and os.path.exists(img_path):\n",
    "            with open(img_path,\"rb\") as img_file:\n",
    "                await update.message.reply_photo(photo = img_file)\n",
    "\n",
    "\n",
    "    else:\n",
    "        await update.message.reply_text(\"Sorry, I can't talk right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "\n",
    "def start_bot():\n",
    "    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()\n",
    "\n",
    "    app.add_handler(MesssageHandler(filters.VOICE),handle_voice)\n",
    "    app.add_handler(MesssageHandler(filters.PHOTO),handle_photo)\n",
    "    app.add_handler(MesssageHandler(filters.TEXT & ~ filters.COMMAND),handle_text)\n",
    "\n",
    "    print(\"Telegram Bot is ready....\")\n",
    "\n",
    "    app.run_polling()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74782a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "await start_bot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
